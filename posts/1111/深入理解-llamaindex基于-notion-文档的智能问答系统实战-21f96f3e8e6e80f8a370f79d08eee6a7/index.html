<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>xiaoliu's blog</title><meta name=keywords content><meta name=description content="深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战
type: Post
status: Published
date: 2025/06/27
tags: ai, llamaindex, notion
category: 技术分享
在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。
然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。
本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。
什么是 LlamaIndex？
LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。
官网：https://www.llamaindex.ai/

我们的目标

从 Notion 加载文档（不限深度递归）
用 HuggingFace 的嵌入模型做向量化
自定义 DeepSeek LLM 做问答
持久化索引，方便下次直接用


项目结构及代码说明
1. 环境准备

pip install llama-index notion-client python-dotenv requests
并且你需要在 .env 文件里配置："><meta name=author content><link rel=canonical href=https://frank9306.github.io/posts/1111/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-llamaindex%E5%9F%BA%E4%BA%8E-notion-%E6%96%87%E6%A1%A3%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-21f96f3e8e6e80f8a370f79d08eee6a7/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://frank9306.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://frank9306.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frank9306.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://frank9306.github.io/apple-touch-icon.png><link rel=mask-icon href=https://frank9306.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://frank9306.github.io/posts/1111/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-llamaindex%E5%9F%BA%E4%BA%8E-notion-%E6%96%87%E6%A1%A3%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-21f96f3e8e6e80f8a370f79d08eee6a7/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://frank9306.github.io/posts/1111/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-llamaindex%E5%9F%BA%E4%BA%8E-notion-%E6%96%87%E6%A1%A3%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-21f96f3e8e6e80f8a370f79d08eee6a7/"><meta property="og:site_name" content="xiaoliu's blog"><meta property="og:title" content="xiaoliu's blog"><meta property="og:description" content="深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战 type: Post status: Published date: 2025/06/27 tags: ai, llamaindex, notion category: 技术分享
在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。
然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。
本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。
什么是 LlamaIndex？ LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。
官网：https://www.llamaindex.ai/
我们的目标 从 Notion 加载文档（不限深度递归） 用 HuggingFace 的嵌入模型做向量化 自定义 DeepSeek LLM 做问答 持久化索引，方便下次直接用 项目结构及代码说明 1. 环境准备 pip install llama-index notion-client python-dotenv requests 并且你需要在 .env 文件里配置："><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战
type: Post
status: Published
date: 2025/06/27
tags: ai, llamaindex, notion
category: 技术分享
在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。
然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。
本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。
什么是 LlamaIndex？
LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。
官网：https://www.llamaindex.ai/

我们的目标

从 Notion 加载文档（不限深度递归）
用 HuggingFace 的嵌入模型做向量化
自定义 DeepSeek LLM 做问答
持久化索引，方便下次直接用


项目结构及代码说明
1. 环境准备

pip install llama-index notion-client python-dotenv requests
并且你需要在 .env 文件里配置："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frank9306.github.io/posts/"},{"@type":"ListItem","position":2,"name":"","item":"https://frank9306.github.io/posts/1111/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-llamaindex%E5%9F%BA%E4%BA%8E-notion-%E6%96%87%E6%A1%A3%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-21f96f3e8e6e80f8a370f79d08eee6a7/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战 type: Post status: Published date: 2025/06/27 tags: ai, llamaindex, notion category: 技术分享\n在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。\n然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。\n本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。\n什么是 LlamaIndex？ LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。\n官网：https://www.llamaindex.ai/\n我们的目标 从 Notion 加载文档（不限深度递归） 用 HuggingFace 的嵌入模型做向量化 自定义 DeepSeek LLM 做问答 持久化索引，方便下次直接用 项目结构及代码说明 1. 环境准备 pip install llama-index notion-client python-dotenv requests 并且你需要在 .env 文件里配置：\n","keywords":[],"articleBody":"深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战 type: Post status: Published date: 2025/06/27 tags: ai, llamaindex, notion category: 技术分享\n在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。\n然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。\n本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。\n什么是 LlamaIndex？ LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。\n官网：https://www.llamaindex.ai/\n我们的目标 从 Notion 加载文档（不限深度递归） 用 HuggingFace 的嵌入模型做向量化 自定义 DeepSeek LLM 做问答 持久化索引，方便下次直接用 项目结构及代码说明 1. 环境准备 pip install llama-index notion-client python-dotenv requests 并且你需要在 .env 文件里配置：\nNOTION_TOKEN=你的Notion集成Token NOTION_PAGE_ID=你的Notion页面ID DEEPSEEK_API_KEY=你的DeepSeek API密钥 llama_index 默认是使用openai的，但是没买，顺便也看下如何自定义模型。另外NOTION_TOKEN 和 NOTION_PAGE_ID 获取就不在赘述。\n2. Notion 文档加载工具（notion_loader.py） 负责递归抓取 Notion 页面及子页面文本，返回 List[Document]。\nimport os from notion_client import Client from llama_index.core.schema import Document from dotenv import load_dotenv from typing import List load_dotenv() notion = Client(auth=os.getenv(\"NOTION_TOKEN\")) def load_notion_all_docs(page_id: str) -\u003e List[Document]: docs = [] def recurse_blocks(block_id: str): children = notion.blocks.children.list(block_id).get(\"results\", []) texts = [] for block in children: b_type = block[\"type\"] if b_type in [\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\"]: rich_text = block[b_type].get(\"rich_text\", []) if rich_text: texts.append(\"\".join([t.get(\"plain_text\", \"\") for t in rich_text])) elif b_type == \"child_page\": child_page_id = block[\"id\"] docs.extend(load_notion_all_docs(child_page_id)) # 你还可以根据需求扩展处理代码块、引用块等 if texts: docs.append(Document(text=\"\\n\".join(texts), metadata={\"source\": block_id})) recurse_blocks(page_id) return docs 3. DeepSeek 自定义 LLM 适配器（deepseek_llm.py） 实现了 CustomLLM，对接 DeepSeek API，支持同步和流式返回。\nfrom typing import Any, Generator import json import requests from pydantic import Field from llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata from llama_index.core.llms.callbacks import llm_completion_callback class DeepSeekLLM(CustomLLM): api_key: str = Field(...) api_base: str = Field(default=\"https://api.deepseek.com/v1\") model_name: str = Field(default=\"deepseek-chat\") temperature: float = Field(default=0.7) max_tokens: int = Field(default=1024) @property def metadata(self) -\u003e LLMMetadata: return LLMMetadata( context_window=8192, num_output=self.max_tokens, model_name=self.model_name, ) def _request(self, prompt: str, stream: bool = False): headers = { \"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\", } body = { \"model\": self.model_name, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": self.temperature, \"max_tokens\": self.max_tokens, \"stream\": stream, } res = requests.post( f\"{self.api_base}/chat/completions\", headers=headers, json=body, stream=stream, timeout=60, ) res.raise_for_status() return res @llm_completion_callback() def complete(self, prompt: str, **kwargs: Any) -\u003e CompletionResponse: res = self._request(prompt, stream=False) data = res.json() message = data[\"choices\"][0][\"message\"][\"content\"] return CompletionResponse(text=message) @llm_completion_callback() def stream_complete(self, prompt: str, **kwargs: Any) -\u003e CompletionResponseGen: res = self._request(prompt, stream=True) partial = \"\" for line in res.iter_lines(decode_unicode=True): if line.startswith(\"data: \"): line = line[len(\"data: \"):] if line.strip() == \"[DONE]\": break try: data = json.loads(line) delta = data[\"choices\"][0][\"delta\"].get(\"content\", \"\") partial += delta yield CompletionResponse(text=partial, delta=delta) except Exception: continue 4. 主程序（main.py） 完整流程：加载 Notion 文档，构建索引，持久化，循环问答。\nimport os from dotenv import load_dotenv from llama_index.core import StorageContext, VectorStoreIndex, Settings from llama_index.embeddings.huggingface import HuggingFaceEmbedding from deepseek_llm import DeepSeekLLM from notion_loader import load_notion_all_docs load_dotenv() page_id = os.getenv(\"NOTION_PAGE_ID\") deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\") # 配置 LLM 和嵌入模型 Settings.llm = DeepSeekLLM(api_key=deepseek_api_key) Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") def ask(index, query: str): engine = index.as_query_engine(llm=Settings.llm) return engine.query(query) if __name__ == \"__main__\": print(\"📄 开始加载 Notion 文档...\") docs = load_notion_all_docs(page_id) print(f\"📄 加载文档完成，文档数量：{len(docs)}\") print(\"🔍 开始构建索引...\") index = VectorStoreIndex.from_documents(docs) index.storage_context.persist(persist_dir=\"storage\") print(\"✅ 索引构建完成并持久化\") while True: q = input(\"🔍 请输入问题（输入 exit 或 quit 退出）：\") if q.strip().lower() in [\"exit\", \"quit\"]: break answer = ask(index, q) print(\"💬 回答：\", answer) 示例测试数据 为了验证我们的问答系统效果，用GPT Mock了两段内容，我们在 Notion 中创建了两个子页面，内容如下（以下是示意内容节选）：\n📄 子页面一： 李雷的爸爸叫张三，妈妈叫王芳，姐姐叫李梅。 韩梅梅的男朋友是李雷。 张三是一名建筑工程师，毕业于清华大学土木工程系。 王芳是一位医生，曾在北京协和医院工作十年。 王小明出生于2000年5月4日，是一名程序员，擅长Python和Go语言。 王小明在2023年加入了字节跳动，担任后端开发工程师。 大白是一只三岁的萨摩耶犬，它的主人是韩梅梅。 韩梅梅家住在北京市朝阳区幸福路88号。 📄 子页面二： 《微积分入门》是由陈建华教授编写的一本数学教材，适用于大一学生。 《现代操作系统》是由 Andrew S. Tanenbaum 编写的，被广泛用于计算机专业课程。 百度是由李彦宏于2000年创建的，总部位于北京市海淀区。 阿里巴巴的创始人是马云，成立于1999年，总部位于杭州。 地球是太阳系中的第三颗行星，平均距离太阳约为1.5亿公里。 水的化学式是H₂O，常温下为无色无味液体。 林晓是一名UI设计师，毕业于中国美术学院，目前在腾讯工作，负责微信的界面设计。 林晓的表哥叫赵强，是一位网络安全专家，在阿里云任职。 赵强的妻子是李娜，曾在华为工作，现在是独立安全顾问。 李娜和林晓在2022年共同参与了一个关于“用户隐私保护”的研究项目。 林晓的男朋友叫陈宇，是一名前端工程师，精通Vue和React。 陈宇在2021年从网易跳槽到了美团，目前居住在上海浦东新区。 陈宇和赵强在一次黑客马拉松比赛中认识，当时他们的队伍获得了一等奖。 赵强的父亲赵云，是一位退休军人，曾服役于广州军区。 赵云非常喜欢书法，经常在社区活动中心教老人写毛笔字。 林晓的宠物是一只英短猫，名字叫“灰灰”，今年两岁，最喜欢吃冻干鸡肉。 测试效果 总结 LlamaIndex 让复杂文档结构变得简单 Notion 文档用递归方式采集，结构清晰 DeepSeek LLM 适配接口灵活，能用自定义模型 向量索引结合嵌入模型，大幅提升问答质量 持久化索引避免重复构建，提高效率 ","wordCount":"498","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://frank9306.github.io/posts/1111/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-llamaindex%E5%9F%BA%E4%BA%8E-notion-%E6%96%87%E6%A1%A3%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-21f96f3e8e6e80f8a370f79d08eee6a7/"},"publisher":{"@type":"Organization","name":"xiaoliu's blog","logo":{"@type":"ImageObject","url":"https://frank9306.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://frank9306.github.io/ accesskey=h title="xiaoliu's blog (Alt + H)">xiaoliu's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://frank9306.github.io/ title=Services><span>Services</span></a></li><li><a href=https://frank9306.github.io/ title=About><span>About</span></a></li><li><a href=https://frank9306.github.io/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent"></h1><div class=post-meta></div></header><div class=post-content><h1 id=深入理解-llamaindex基于-notion-文档的智能问答系统实战>深入理解 LlamaIndex：基于 Notion 文档的智能问答系统实战<a hidden class=anchor aria-hidden=true href=#深入理解-llamaindex基于-notion-文档的智能问答系统实战>#</a></h1><p>type: Post
status: Published
date: 2025/06/27
tags: ai, llamaindex, notion
category: 技术分享</p><p>在人工智能日益融入开发者工具链的今天，我最初计划使用 LangChain 构建一个基于文档的智能问答系统。LangChain 作为一个强大的框架，能够将大型语言模型（LLMs）与各种数据源和应用场景连接起来，看起来是个不错的选择。</p><p>然而，在深入研究和实践过程中，我发现了 LlamaIndex（前身为 GPT Index）这个更为专注于知识检索和问答的框架。LlamaIndex 在处理文档索引、构建知识库以及生成上下文相关回答方面，提供了更为精细和直观的工具。尤其是对于我这个需要基于 Notion 文档构建智能问答系统的场景，LlamaIndex 提供了更加无缝的集成体验。</p><p>本文将分享我从 LangChain 转向 LlamaIndex 的经历，以及如何利用 LlamaIndex 构建一个能够理解、检索并回答 Notion 文档内容的智能问答系统。无论你是 AI 开发新手，还是寻找更高效文档处理方案的资深开发者，希望这篇实战分享能给你带来一些启发。</p><h2 id=什么是-llamaindex>什么是 LlamaIndex？<a hidden class=anchor aria-hidden=true href=#什么是-llamaindex>#</a></h2><p>LlamaIndex（原名 GPT Index）是一个开源框架，帮你把各种格式的文档变成结构化的索引，方便大语言模型（LLM）用更少的上下文轻松理解和回答问题。简单说就是：帮你把大海捞针变成小池塘钓鱼。</p><p>官网：<a href=https://www.llamaindex.ai/>https://www.llamaindex.ai/</a></p><hr><h2 id=我们的目标>我们的目标<a hidden class=anchor aria-hidden=true href=#我们的目标>#</a></h2><ul><li>从 Notion 加载文档（不限深度递归）</li><li>用 HuggingFace 的嵌入模型做向量化</li><li>自定义 DeepSeek LLM 做问答</li><li>持久化索引，方便下次直接用</li></ul><hr><h2 id=项目结构及代码说明>项目结构及代码说明<a hidden class=anchor aria-hidden=true href=#项目结构及代码说明>#</a></h2><h3 id=1-环境准备>1. 环境准备<a hidden class=anchor aria-hidden=true href=#1-环境准备>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>pip install llama-index notion-client python-dotenv requests
</span></span></code></pre></div><p>并且你需要在 <code>.env</code> 文件里配置：</p><pre tabindex=0><code>
NOTION_TOKEN=你的Notion集成Token
NOTION_PAGE_ID=你的Notion页面ID
DEEPSEEK_API_KEY=你的DeepSeek API密钥
</code></pre><p><code>llama_index</code> 默认是使用openai的，但是没买，顺便也看下如何自定义模型。另外<code>NOTION_TOKEN</code> 和 <code>NOTION_PAGE_ID</code> 获取就不在赘述。</p><hr><h3 id=2-notion-文档加载工具notion_loaderpy>2. Notion 文档加载工具（<code>notion_loader.py</code>）<a hidden class=anchor aria-hidden=true href=#2-notion-文档加载工具notion_loaderpy>#</a></h3><p>负责递归抓取 Notion 页面及子页面文本，返回 <code>List[Document]</code>。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> notion_client <span style=color:#f92672>import</span> Client
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> llama_index.core.schema <span style=color:#f92672>import</span> Document
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> List
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>notion <span style=color:#f92672>=</span> Client(auth<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;NOTION_TOKEN&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_notion_all_docs</span>(page_id: str) <span style=color:#f92672>-&gt;</span> List[Document]:
</span></span><span style=display:flex><span>    docs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recurse_blocks</span>(block_id: str):
</span></span><span style=display:flex><span>        children <span style=color:#f92672>=</span> notion<span style=color:#f92672>.</span>blocks<span style=color:#f92672>.</span>children<span style=color:#f92672>.</span>list(block_id)<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;results&#34;</span>, [])
</span></span><span style=display:flex><span>        texts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> block <span style=color:#f92672>in</span> children:
</span></span><span style=display:flex><span>            b_type <span style=color:#f92672>=</span> block[<span style=color:#e6db74>&#34;type&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> b_type <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;paragraph&#34;</span>, <span style=color:#e6db74>&#34;heading_1&#34;</span>, <span style=color:#e6db74>&#34;heading_2&#34;</span>, <span style=color:#e6db74>&#34;heading_3&#34;</span>]:
</span></span><span style=display:flex><span>                rich_text <span style=color:#f92672>=</span> block[b_type]<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;rich_text&#34;</span>, [])
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> rich_text:
</span></span><span style=display:flex><span>                    texts<span style=color:#f92672>.</span>append(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join([t<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;plain_text&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>) <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> rich_text]))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> b_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;child_page&#34;</span>:
</span></span><span style=display:flex><span>                child_page_id <span style=color:#f92672>=</span> block[<span style=color:#e6db74>&#34;id&#34;</span>]
</span></span><span style=display:flex><span>                docs<span style=color:#f92672>.</span>extend(load_notion_all_docs(child_page_id))
</span></span><span style=display:flex><span>            <span style=color:#75715e># 你还可以根据需求扩展处理代码块、引用块等</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> texts:
</span></span><span style=display:flex><span>            docs<span style=color:#f92672>.</span>append(Document(text<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(texts), metadata<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;source&#34;</span>: block_id}))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    recurse_blocks(page_id)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> docs
</span></span></code></pre></div><hr><h3 id=3-deepseek-自定义-llm-适配器deepseek_llmpy>3. DeepSeek 自定义 LLM 适配器（<code>deepseek_llm.py</code>）<a hidden class=anchor aria-hidden=true href=#3-deepseek-自定义-llm-适配器deepseek_llmpy>#</a></h3><p>实现了 <code>CustomLLM</code>，对接 DeepSeek API，支持同步和流式返回。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Any, Generator
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> Field
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> llama_index.core.llms <span style=color:#f92672>import</span> CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> llama_index.core.llms.callbacks <span style=color:#f92672>import</span> llm_completion_callback
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DeepSeekLLM</span>(CustomLLM):
</span></span><span style=display:flex><span>    api_key: str <span style=color:#f92672>=</span> Field(<span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>    api_base: str <span style=color:#f92672>=</span> Field(default<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;https://api.deepseek.com/v1&#34;</span>)
</span></span><span style=display:flex><span>    model_name: str <span style=color:#f92672>=</span> Field(default<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;deepseek-chat&#34;</span>)
</span></span><span style=display:flex><span>    temperature: float <span style=color:#f92672>=</span> Field(default<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>)
</span></span><span style=display:flex><span>    max_tokens: int <span style=color:#f92672>=</span> Field(default<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@property</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>metadata</span>(self) <span style=color:#f92672>-&gt;</span> LLMMetadata:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> LLMMetadata(
</span></span><span style=display:flex><span>            context_window<span style=color:#f92672>=</span><span style=color:#ae81ff>8192</span>,
</span></span><span style=display:flex><span>            num_output<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>max_tokens,
</span></span><span style=display:flex><span>            model_name<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model_name,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_request</span>(self, prompt: str, stream: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>        headers <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;Authorization&#34;</span>: <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Bearer </span><span style=color:#e6db74>{</span>self<span style=color:#f92672>.</span>api_key<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;Content-Type&#34;</span>: <span style=color:#e6db74>&#34;application/json&#34;</span>,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        body <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;model&#34;</span>: self<span style=color:#f92672>.</span>model_name,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;messages&#34;</span>: [{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}],
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;temperature&#34;</span>: self<span style=color:#f92672>.</span>temperature,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;max_tokens&#34;</span>: self<span style=color:#f92672>.</span>max_tokens,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;stream&#34;</span>: stream,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>self<span style=color:#f92672>.</span>api_base<span style=color:#e6db74>}</span><span style=color:#e6db74>/chat/completions&#34;</span>,
</span></span><span style=display:flex><span>            headers<span style=color:#f92672>=</span>headers,
</span></span><span style=display:flex><span>            json<span style=color:#f92672>=</span>body,
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>=</span>stream,
</span></span><span style=display:flex><span>            timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>60</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        res<span style=color:#f92672>.</span>raise_for_status()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> res
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@llm_completion_callback</span>()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>complete</span>(self, prompt: str, <span style=color:#f92672>**</span>kwargs: Any) <span style=color:#f92672>-&gt;</span> CompletionResponse:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_request(prompt, stream<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> res<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>        message <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;choices&#34;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;message&#34;</span>][<span style=color:#e6db74>&#34;content&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> CompletionResponse(text<span style=color:#f92672>=</span>message)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@llm_completion_callback</span>()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>stream_complete</span>(self, prompt: str, <span style=color:#f92672>**</span>kwargs: Any) <span style=color:#f92672>-&gt;</span> CompletionResponseGen:
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_request(prompt, stream<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        partial <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> res<span style=color:#f92672>.</span>iter_lines(decode_unicode<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> line<span style=color:#f92672>.</span>startswith(<span style=color:#e6db74>&#34;data: &#34;</span>):
</span></span><span style=display:flex><span>                line <span style=color:#f92672>=</span> line[len(<span style=color:#e6db74>&#34;data: &#34;</span>):]
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> line<span style=color:#f92672>.</span>strip() <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;[DONE]&#34;</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                data <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(line)
</span></span><span style=display:flex><span>                delta <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;choices&#34;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;delta&#34;</span>]<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;content&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>                partial <span style=color:#f92672>+=</span> delta
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>yield</span> CompletionResponse(text<span style=color:#f92672>=</span>partial, delta<span style=color:#f92672>=</span>delta)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span></code></pre></div><hr><h3 id=4-主程序mainpy>4. 主程序（<code>main.py</code>）<a hidden class=anchor aria-hidden=true href=#4-主程序mainpy>#</a></h3><p>完整流程：加载 Notion 文档，构建索引，持久化，循环问答。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dotenv <span style=color:#f92672>import</span> load_dotenv
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> llama_index.core <span style=color:#f92672>import</span> StorageContext, VectorStoreIndex, Settings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> llama_index.embeddings.huggingface <span style=color:#f92672>import</span> HuggingFaceEmbedding
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> deepseek_llm <span style=color:#f92672>import</span> DeepSeekLLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> notion_loader <span style=color:#f92672>import</span> load_notion_all_docs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>load_dotenv()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>page_id <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;NOTION_PAGE_ID&#34;</span>)
</span></span><span style=display:flex><span>deepseek_api_key <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;DEEPSEEK_API_KEY&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 配置 LLM 和嵌入模型</span>
</span></span><span style=display:flex><span>Settings<span style=color:#f92672>.</span>llm <span style=color:#f92672>=</span> DeepSeekLLM(api_key<span style=color:#f92672>=</span>deepseek_api_key)
</span></span><span style=display:flex><span>Settings<span style=color:#f92672>.</span>embed_model <span style=color:#f92672>=</span> HuggingFaceEmbedding(model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;BAAI/bge-small-en-v1.5&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ask</span>(index, query: str):
</span></span><span style=display:flex><span>    engine <span style=color:#f92672>=</span> index<span style=color:#f92672>.</span>as_query_engine(llm<span style=color:#f92672>=</span>Settings<span style=color:#f92672>.</span>llm)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> engine<span style=color:#f92672>.</span>query(query)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;📄 开始加载 Notion 文档...&#34;</span>)
</span></span><span style=display:flex><span>    docs <span style=color:#f92672>=</span> load_notion_all_docs(page_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;📄 加载文档完成，文档数量：</span><span style=color:#e6db74>{</span>len(docs)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;🔍 开始构建索引...&#34;</span>)
</span></span><span style=display:flex><span>    index <span style=color:#f92672>=</span> VectorStoreIndex<span style=color:#f92672>.</span>from_documents(docs)
</span></span><span style=display:flex><span>    index<span style=color:#f92672>.</span>storage_context<span style=color:#f92672>.</span>persist(persist_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;storage&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;✅ 索引构建完成并持久化&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        q <span style=color:#f92672>=</span> input(<span style=color:#e6db74>&#34;🔍 请输入问题（输入 exit 或 quit 退出）：&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> q<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>lower() <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;exit&#34;</span>, <span style=color:#e6db74>&#34;quit&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        answer <span style=color:#f92672>=</span> ask(index, q)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;💬 回答：&#34;</span>, answer)
</span></span></code></pre></div><hr><h2 id=示例测试数据>示例测试数据<a hidden class=anchor aria-hidden=true href=#示例测试数据>#</a></h2><p>为了验证我们的问答系统效果，用GPT Mock了两段内容，我们在 Notion 中创建了两个子页面，内容如下（以下是示意内容节选）：</p><h3 id=-子页面一>📄 子页面一：<a hidden class=anchor aria-hidden=true href=#-子页面一>#</a></h3><pre tabindex=0><code>
李雷的爸爸叫张三，妈妈叫王芳，姐姐叫李梅。
韩梅梅的男朋友是李雷。
张三是一名建筑工程师，毕业于清华大学土木工程系。
王芳是一位医生，曾在北京协和医院工作十年。

王小明出生于2000年5月4日，是一名程序员，擅长Python和Go语言。
王小明在2023年加入了字节跳动，担任后端开发工程师。

大白是一只三岁的萨摩耶犬，它的主人是韩梅梅。
韩梅梅家住在北京市朝阳区幸福路88号。
</code></pre><h3 id=-子页面二>📄 子页面二：<a hidden class=anchor aria-hidden=true href=#-子页面二>#</a></h3><pre tabindex=0><code>
《微积分入门》是由陈建华教授编写的一本数学教材，适用于大一学生。
《现代操作系统》是由 Andrew S. Tanenbaum 编写的，被广泛用于计算机专业课程。

百度是由李彦宏于2000年创建的，总部位于北京市海淀区。
阿里巴巴的创始人是马云，成立于1999年，总部位于杭州。

地球是太阳系中的第三颗行星，平均距离太阳约为1.5亿公里。
水的化学式是H₂O，常温下为无色无味液体。

林晓是一名UI设计师，毕业于中国美术学院，目前在腾讯工作，负责微信的界面设计。
林晓的表哥叫赵强，是一位网络安全专家，在阿里云任职。

赵强的妻子是李娜，曾在华为工作，现在是独立安全顾问。
李娜和林晓在2022年共同参与了一个关于“用户隐私保护”的研究项目。

林晓的男朋友叫陈宇，是一名前端工程师，精通Vue和React。
陈宇在2021年从网易跳槽到了美团，目前居住在上海浦东新区。

陈宇和赵强在一次黑客马拉松比赛中认识，当时他们的队伍获得了一等奖。

赵强的父亲赵云，是一位退休军人，曾服役于广州军区。
赵云非常喜欢书法，经常在社区活动中心教老人写毛笔字。

林晓的宠物是一只英短猫，名字叫“灰灰”，今年两岁，最喜欢吃冻干鸡肉。
</code></pre><hr><h2 id=测试效果>测试效果<a hidden class=anchor aria-hidden=true href=#测试效果>#</a></h2><p><img alt=image.png loading=lazy src=image.png></p><hr><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><ul><li>LlamaIndex 让复杂文档结构变得简单</li><li>Notion 文档用递归方式采集，结构清晰</li><li>DeepSeek LLM 适配接口灵活，能用自定义模型</li><li>向量索引结合嵌入模型，大幅提升问答质量</li><li>持久化索引避免重复构建，提高效率</li></ul><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://frank9306.github.io/>xiaoliu's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>